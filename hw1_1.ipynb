{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a88b4ec56484966",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **a_tensor_initialization.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:27:47.931516Z",
     "start_time": "2024-09-20T08:27:47.915718Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "78c850fc11cf9705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:27:49.188336Z",
     "start_time": "2024-09-20T08:27:49.175372Z"
    }
   },
   "source": [
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')    ##대문자로 시작하는 텐서는 float32\n",
    "print(t1.dtype)   # >>> torch.float32         ##텐서의 타입\n",
    "print(t1.device)  # >>> cpu                   ##텐서가 위치한 장치(cpu or gpu)\n",
    "print(t1.requires_grad)  # >>> False          ##텐서에 대한 기울기가 계산되지 않음 -> 성능 up\n",
    "print(t1.size())  # torch.Size([3])           ##텐서의 사이즈\n",
    "print(t1.shape)   # torch.Size([3])           ##텐서의 사이즈\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))       ##gpu로 텐서를 올리는 방법\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()                         ##같은 의미의 짧은 코드\n",
    "t1_cpu = t1.cpu()                             ##cpu로 텐서 올기기\n",
    "\n",
    "print(\"#\" * 50, 1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "409ac43337032c35",
   "metadata": {},
   "source": "- torch.Tensor() 함수는 무조건 자료형이 float32이다."
  },
  {
   "cell_type": "code",
   "id": "76070404-bd13-4f00-8ab9-8ed035118f16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:27:58.664126Z",
     "start_time": "2024-09-20T08:27:58.645978Z"
    }
   },
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')    ##소문자로 시작하는 텐서는 리스트 내 원소 타입의 자료형을 가짐\n",
    "print(t2.dtype)  # >>> torch.int64            ##1, 2, 3은 정수이므로 int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "16ee1269bcb99818",
   "metadata": {},
   "source": [
    "- torch.tensor() 함수는 주어진 리스트 내 원소 타입의 자료형을 가진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0e4f2bab78fc5b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:28:07.012916Z",
     "start_time": "2024-09-20T08:28:06.939007Z"
    }
   },
   "source": [
    "a1 = torch.tensor(1)  # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)    ##그냥 숫자 -> size []인 0차원,  [] 안에 있어야 텐서임\n",
    "\n",
    "a2 = torch.tensor([1])  # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)    ##size [1]인 1차원 텐서\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])  # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)    ##size [5]인 1차원 텐서\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])  # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)    ##size [5, 1]인 2차원 텐서\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)    ##size [3, 2]인 2차원 텐서\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)    ##size [3, 2, 1]인 3차원 텐서\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)    ##size [3, 1, 2, 1]인 4차원 텐서\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)    ##size [3, 1, 2, 3]인 4차원 텐서\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)    ##size [3, 1, 2, 3, 1]인 5차원 텐서\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)    ##size [4, 5]인 2차원 텐서\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)    ##size [4, 1, 5]인 3차원 텐서\n",
    "                              ##콤마의 개수로 판단하면 편함\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "                              ##동일한 차원은 반드시 같은 사이즈를 가져야하므로 error 발생"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 65\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(a10\u001B[38;5;241m.\u001B[39mshape, a10\u001B[38;5;241m.\u001B[39mndim)    \u001B[38;5;66;03m##size [4, 1, 5]인 3차원 텐서\u001B[39;00m\n\u001B[0;32m     63\u001B[0m                               \u001B[38;5;66;03m##콤마의 개수로 판단하면 편함\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m a11 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;49;00m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fece6955795bdc1f",
   "metadata": {},
   "source": [
    "- 콤마의 개수로 텐서의 차원을 판단하면 편하고, 동일한 차원은 반드시 같은 사이즈를 가져야한다.\n",
    "- 따라서 마지막 a11은 에러가 발생한다. 그 전 텐서들은 잘 출력됐고, 원 코드에서 에러 발생 부분에 주석처리가 안되어있었기 때문에 코드를 그대로 유지하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a39a59a231e8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **b_tensor_initialization_copy.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "efd2f7f1338fad20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:28:44.073015Z",
     "start_time": "2024-09-20T08:28:44.068030Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "4e9bcefe99db7b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:28:46.410266Z",
     "start_time": "2024-09-20T08:28:46.387520Z"
    }
   },
   "source": [
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)           ##리스트 l1을 torch.Tensor()로 변환 (dtype: float32)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)           ##리스트 l2를 torch.tensor()로 변환 (데이터 복사, dtype: int64)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)        ##리스트 l3를 torch.as_tensor()로 변환 (파이썬 리스트의 경우 데이터 복사)\n",
    "\n",
    "l1[0] = 100                     ##l1, l2, l3의 첫번째 요소를 100으로 변경\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)                       ##t1은 l1과 무관한 독립적인 텐서 (데이터 복사, 영향 X)\n",
    "print(t2)                       ##t2는 l2와 무관한 독립적인 텐서 (데이터 복사, 영향 X)\n",
    "print(t3)                       ##t3는 l3와 무관한 독립적인 텐서 (데이터 복사, 영향 X)\n",
    "\n",
    "print(\"#\" * 100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "13a7ca69b6e288d0",
   "metadata": {},
   "source": [
    "- torch.as_tensor()는 넘파이 배열이나 텐서를 입력으로 받을 때 메모리 참조를 시도하지만, 파이썬 리스트를 입력으로 받을 때는 데이터를 복사한다. 그래서 l3를 변경해도 t3에는 그 변화가 반영되지 않는다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95e750c279696357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:28:52.741325Z",
     "start_time": "2024-09-20T08:28:52.725370Z"
    }
   },
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)           ##넘파이 배열 l4를 torch.Tensor()로 변환 (데이터 복사, dtype: float32)\n",
    "\n",
    "l5 = np.array([1, 2, 3])        \n",
    "t5 = torch.tensor(l5)           ##넘파이 배열 l5를 torch.tensor()로 변환 (데이터 복사, dtype: int64)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)        ##넘파이 배열 l6를 torch.as_tensor()로 변환 (데이터 참조, dtype: int64)\n",
    "\n",
    "l4[0] = 100                     ### l4, l5, l6의 첫 번째 요소를 100으로 변경\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)                       ##t4는 l4와 무관한 독립적인 텐서 (데이터 복사, 영향 X)\n",
    "print(t5)                       ##t5는 l5와 무관한 독립적인 텐서 (데이터 복사, 영향 X)\n",
    "print(t6)                       ##t6는 넘파이 배열 l6와 메모리를 공유하므로 l6의 변경이 t6에도 반영됨\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "7ce6a95c6c6dd965",
   "metadata": {},
   "source": [
    "- torch.tensor(): 데이터를 항상 복사하여 새로운 텐서를 생성한다. 원본 데이터와 독립적인 텐서를 생성하고자 할 때 사용한다.\n",
    "- torch.as_tensor(): 데이터 복사 없이 참조를 사용해 텐서를 생성한다. 메모리를 효율적으로 사용하거나 원본 데이터를 공유하고자 할 때 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe824fa4a12142",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **c_tensor_initialization_constant_values.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "c568f00c4fb9726d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:02.876069Z",
     "start_time": "2024-09-20T08:29:02.859115Z"
    }
   },
   "source": [
    "import torch\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9e6635c91963beb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:04.480194Z",
     "start_time": "2024-09-20T08:29:04.468226Z"
    }
   },
   "source": [
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)      ##크기가 5인 모든 값이 1인 텐서 생성 (dtype: float32)  \n",
    "t1_like = torch.ones_like(input=t1)                 ##t1과 동일한 shape을 가지는 모든 값이 1인 텐서 생성, dtype과 device도 동일 (float32, cpu)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "b754af87df0940a4",
   "metadata": {},
   "source": [
    "- torch.ones(*size)는 크기가 n인 모든 값이 1인 텐서를 생성한다.\n",
    "- torch.ones_like(input_tensor)는 input_tensor의 shape, dtype, device 속성을 그대로 복사하여 모든 값이 1인 새로운 텐서를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "a67f445338fb8879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:08.275917Z",
     "start_time": "2024-09-20T08:29:08.261900Z"
    }
   },
   "source": [
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)        ##크기가 6인 모든 값이 0인 텐서 생성 (dtype: float32)\n",
    "t2_like = torch.zeros_like(input=t2)                    ##t2와 동일한 shape을 가지는 모든 값이 0인 텐서 생성, dtype과 device도 동일 (float32, cpu)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "de3813baf30b878e",
   "metadata": {},
   "source": [
    "- torch.zeros(*size)는 크기가 n인 모든 값이 0인 텐서를 생성한다.\n",
    "- torch.zeros_like(input_tensor)는 input_tensor의 shape, dtype, device 속성을 그대로 복사하여 모든 값이 0인 새로운 텐서를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "68e81fe143803ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:35.033768Z",
     "start_time": "2024-09-20T08:29:35.022797Z"
    }
   },
   "source": [
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)    ##크기가 4인 텐서 생성, 메모리만 할당하고 초기화는 하지 않음\n",
    "t3_like = torch.empty_like(input=t3)                ##t3와 동일한 shape을 가지는 텐서 생성, t3와 똑같이 초기화는 안되어 있음\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 3., 0.])\n",
      "tensor([1.0000, 4.4766, 3.0000, 0.0000])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "a392aebb18a3c0ad",
   "metadata": {},
   "source": [
    "- torch.empty(*size): 크기가 n인 텐서를 생성하지만 초기화하지 않는다. 즉, 텐서의 값은 임의의 값이나 이전 메모리 상태에 따라 결정된다.\n",
    "- torch.empty_like(input_tensor): t3와 동일한 크기 및 dtype을 가진 텐서를 생성하되, 역시 초기화되지 않은 상태로 텐서를 생성한다.\n",
    "- torch.empty()나 torch.empty_like()에서 출력되는 값은 비결정적이며, 이전 메모리 상태에 따라 달라진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbca560301049066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:41.745903Z",
     "start_time": "2024-09-20T08:29:41.727954Z"
    }
   },
   "source": [
    "t4 = torch.eye(n=3)         ##n * n 크기의 단위 행렬 생성(대각선 1, 나머지 0)\n",
    "print(t4)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "181f129289a49c59",
   "metadata": {},
   "source": [
    "- torch.eye(n): 크기가 n * n인 단위 행렬을 생성한다. 단위 행렬은 대각선에 있는 값이 모두 1이고, 나머지 값은 0이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4def8702f20a7",
   "metadata": {},
   "source": [
    "# **d_tensor_initialization_random_values.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "70c195f746b4a196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:45.650490Z",
     "start_time": "2024-09-20T08:29:45.636306Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "dfddc57c-8a45-49cc-8caf-1fb2335a9033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:51.922973Z",
     "start_time": "2024-09-20T08:29:51.909955Z"
    }
   },
   "source": [
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)           ##10 이상 20 미만의 정수 중에서 랜덤 값을 가지는 1x2 텐서 생성\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)           ##0과 1 사이에서 균등분포로 랜덤 값을 가지는 1x3 텐서 생성\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)           ##평균이 0, 표준편차가 1인 정규분포에서 랜덤 값을 가지는 1x3 텐서 생성"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 14]])\n",
      "tensor([[0.7178, 0.4226, 0.7910]])\n",
      "tensor([[-0.0641,  0.4824,  0.1907]])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- torch.randint(low, high, size): low 이상 high 미만의 정수에서 무작위 값을 추출하여 size 크기의 정수형 텐서를 생성한다.\n",
    "- torch.rand(size): 균등 분포인 0과 1 사이의 무작위 값을 추출하여 size 크기의 텐서를 생성한다.\n",
    "- torch.randn(size): 정규 분포(평균=0, 표준편차=1)에서 무작위 값을 추출하여 size 크기의 텐서를 생성한다."
   ],
   "id": "3bf0d4af8fc77a54"
  },
  {
   "cell_type": "code",
   "id": "b1f226fa-63c6-4bdd-939a-9c30b9058ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:29:58.634487Z",
     "start_time": "2024-09-20T08:29:58.614800Z"
    }
   },
   "source": [
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)           ##평균이 10.0, 표준편차가 1.0인 정규 분포에서 무작위 값을 가지는 3x2 텐서 생성\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)           ##0.0에서 5.0까지 균등하게 나누어 3개의 값을 가지는 텐서 생성\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)           ##0부터 4까지의 정수를 순차적으로 가지는 텐서 생성\n",
    "\n",
    "print(\"#\" * 30)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.4770,  9.6105],\n",
      "        [ 9.4288,  9.1313],\n",
      "        [11.4126,  9.5816]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- torch.normal(mean, std, size): 평균이 mean, 표준편차가 std인 정규 분포에서 무작위 값을 추출하여 size 크기의 텐서를 만든다.\n",
    "- torch.linspace(start, end, steps): start에서 end까지의 구간을 steps개의 값으로 균등하게 나누어 텐서를 생성한다. (start-end 구간을 steps등분한다고 생각)\n",
    "- torch.arange(start, end, steps): start부터 (end-1)까지의 정수를 순서대로 가지는 1차원 텐서를 생성한다. 위 코드와 같이 end만 지정한다면, start = 0, steps = 1이 default 값이다. "
   ],
   "id": "8d5da45c673f097f"
  },
  {
   "cell_type": "code",
   "id": "f5098d8447a51617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:46:51.972578Z",
     "start_time": "2024-09-20T08:46:51.945650Z"
    }
   },
   "source": [
    "torch.manual_seed(1729)     ##난수 생성기의 시드를 고정하여 동일한 무작위 값이 생성되도록 설정\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)              ##시드 1729의 첫번째 랜덤 텐서\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)              ##시드 1729의 두번째 랜덤 텐서\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)     ##다시 난수 생성기의 시드를 위와 같은 1729로 고정\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)              ##위와 똑같이 첫번째 랜덤 텐서의 결과가 나옴\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)              ##위와 똑같이 두번째 랜덤 텐서의 결과가 나옴"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.manual_seed(seed): 난수 생성기의 시드를 고정하여 실행할 때마다 동일한 난수 값을 생성한다. 시드를 똑같이 설정하면 같은 결과를 다시 얻을 수 있다.",
   "id": "6cf927a0871d4261"
  },
  {
   "cell_type": "markdown",
   "id": "ba924258bafd4f90",
   "metadata": {},
   "source": [
    "# **e_tensor_type_conversion.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "44625f975e4f54e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:49:20.348229Z",
     "start_time": "2024-09-20T08:49:20.334267Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "e157194b7455af93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:52:47.190823Z",
     "start_time": "2024-09-20T08:52:47.170410Z"
    }
   },
   "source": [
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)              ##default 자료형: float32\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)                    ##int16 자료형의 텐서로 생성\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)                    ##[2,3] 크기의 0~20 사이의 값을 가지는 float64 텐서 생성\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)                    ##b 텐서를 int32로 변환하여 d 텐서 생성"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- .to(torch.xxxx) 또는 dtype=torch.xxxx 옵션을 통해 텐서의 자료형을 설정할 수 있다.",
   "id": "171877dff8cefa91"
  },
  {
   "cell_type": "code",
   "id": "455a59d7909e1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T09:01:33.664800Z",
     "start_time": "2024-09-20T09:01:33.644806Z"
    }
   },
   "source": [
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)        ##double(float64) 타입의 텐서 생성\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)    ##short(int16) 타입의 텐서 생성\n",
    "\n",
    "print(double_d.dtype)   \n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)    ##double(float64) 타입의 텐서 생성\n",
    "short_g = double_f.to(torch.short)              ##short(int16) 타입의 텐서 생성\n",
    "print((double_f * short_g).dtype)               ##두 텐서의 곱셈 연산 결과는 더 높은 정밀도를 가지는 자료형(double)으로 정해짐"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- torch.ones(), torch.zeros(), torch.tensor() 등을 통해 다양한 텐서를 생성하고, dtype을 지정하여 자료형을 설정할 수 있다.\n",
    "- .double(), .short()는 텐서를 double 또는 short 자료형으로 변환할 수 있다.\n",
    "- .to()와 .type()은 자료형을 명시적으로 변환하는 다른 방법이다.\n",
    "- 두 텐서의 곱셈 연산 결과는 더 높은 정밀도를 가지는 자료형으로 정해진다."
   ],
   "id": "3311dbf3adcce1ea"
  },
  {
   "cell_type": "markdown",
   "id": "70e592999c883b39",
   "metadata": {},
   "source": [
    "# **f_tensor_operations.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d35ab0ea291153a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:13:31.889674Z",
     "start_time": "2024-09-20T12:13:31.877706Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f183a890c4adfa1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:18:37.107129Z",
     "start_time": "2024-09-20T12:18:37.095120Z"
    }
   },
   "source": [
    "t1 = torch.ones(size=(2, 3))            ##  [[1., 1., 1.],\n",
    "                                        ##   [1., 1., 1.]]\n",
    "\n",
    "t2 = torch.ones(size=(2, 3))            ##  [[1., 1., 1.],\n",
    "                                        ##   [1., 1., 1.]]\n",
    "\n",
    "t3 = torch.add(t1, t2)              ##텐서의 덧셈\n",
    "t4 = t1 + t2                        ##같은 의미의 코드\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "\n",
    "t5 = torch.sub(t1, t2)              ##텐서의 뺄셈\n",
    "t6 = t1 - t2                        ##같은 의미의 코드\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "\n",
    "t7 = torch.mul(t1, t2)              ##텐서의 곱셈\n",
    "t8 = t1 * t2                        ##같은 의미의 코드\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "\n",
    "t9 = torch.div(t1, t2)              ##텐서의 나눗셈\n",
    "t10 = t1 / t2                       ##같은 의미의 코드\n",
    "print(t9)\n",
    "print(t10)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 두 텐서의 덧셈: torch.add() or +\n",
    "- 두 텐서의 뺄셈: torch.sub() or -\n",
    "- 두 텐서의 곱셈: torch.mul() or *\n",
    "- 두 텐서의 나눗셈: torch.div() or /\n",
    "- 두 텐서가 서로 자료형이 다르면 더 높은 정밀도의 자료형을 따름"
   ],
   "id": "96cd9de47a59b628"
  },
  {
   "cell_type": "markdown",
   "id": "3137c288140a05bf",
   "metadata": {},
   "source": [
    "# **g_tensor_operations_mm.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "77e6c1e0dff456f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:30:29.348846Z",
     "start_time": "2024-09-20T12:30:29.334841Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1defb2f0b6005e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:34:57.688182Z",
     "start_time": "2024-09-20T12:34:57.677211Z"
    }
   },
   "source": [
    "t1 = torch.dot(             ##1차원 텐서간의 dot product 계산\n",
    "    torch.tensor([2, 3]), \n",
    "    torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())        ##2*2 + 3*1 = 7, 그냥 스칼라 값이기 때문에 0차원, size는 []\n",
    "\n",
    "print()                     ##가독성을 위해 추가한 코드\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)       ##2차원 텐서간의 행렬 곱셈\n",
    "print(t4, t4.size())        ##결과는 [2, 2] size의 행렬\n",
    "\n",
    "print()                     ##가독성을 위해 추가한 코드\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)      ##3차원 텐서 간의 배치 행렬 곱셈\n",
    "print(t7.size())            ##결과는 [10, 3, 5] size의 행렬"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "\n",
      "tensor([[ 0.2716,  0.2314],\n",
      "        [-0.1935, -1.3399]]) torch.Size([2, 2])\n",
      "\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- torch.dot(): 1차원 텐서 간의 내적 연산을 수행한다. 결과는 스칼라 값이며, 크기는 없다.\n",
    "- torch.mm(): 2차원 텐서 간의 행렬 곱셈을 수행한다. (n\\*m) by (m\\*p) --> (n\\*p)\n",
    "- torch.bmm(): 3차원 텐서 간의 배치 행렬 곱셈을 수행한다. (b\\*n\\*m) by (b\\*m\\*p) --> (b\\*n\\*p)"
   ],
   "id": "33a86a1527c010fd"
  },
  {
   "cell_type": "markdown",
   "id": "849fc12e2eb94047",
   "metadata": {},
   "source": [
    "# **h_tensor_operations_matmul.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "574ce61f5443a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:53:57.023012Z",
     "start_time": "2024-09-20T12:53:57.006494Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "e073b56b5e2d7047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:53:57.993916Z",
     "start_time": "2024-09-20T12:53:57.966986Z"
    }
   },
   "source": [
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)                     ##크기 [3,]인 랜덤 벡터\n",
    "t2 = torch.randn(3)                     ##크기 [3,]인 랜덤 벡터\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "                                        ##벡터 간의 내적 (스칼라 값, 크기 [])\n",
    "                                        \n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)                  ##크기 [3,4]인 행렬\n",
    "t4 = torch.randn(4)                     ##크기 [4,]인 벡터\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])   \n",
    "                                        ##행렬의 각 행에 대해 벡터의 내적 계산(크기 [3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)              ##크기 [10, 3,4]인 행렬\n",
    "t6 = torch.randn(4)                     ##크기 [4,]인 벡터\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "                                        ##t5의 각 배치는 [3,4] 크기의 행렬이고, t6 벡터와 곱하면 [3,]크기의 벡터가 된다. 각 배치에 대해 [3,] 크기의 벡터 10개가 생성되므로 크기는 [10,3]\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)              ##크기 [10, 3,4]인 행렬\n",
    "t8 = torch.randn(10, 4, 5)              ##크기 [10, 4,5]인 행렬\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "                                        ##bmm 곱셈에 따라 [10, 3, 5]가 된다.\n",
    "                                        ##각 배치끼리의 행렬 곱셈\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)              ##크기 [10, 3,4]인 행렬\n",
    "t10 = torch.randn(4, 5)                 ##크기 [4,5]인 행렬\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n",
    "                                        ##배치 행렬과 일반 행렬간의 곱셈\n",
    "                                        ##t9의 각 배치와 t10간의 행렬 곱셈\n",
    "                                        ##결과 사이즈는 [10, 3, 5]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 벡터 간 곱 (Dot Product): 두 벡터의 내적은 스칼라 값을 반환하며, 크기는 []이다.\n",
    "- 행렬과 벡터 곱: 행렬의 각 행과 벡터의 내적을 통해 새로운 벡터를 만든다.\n",
    "- 배치 행렬 곱: 여러 배치의 행렬을 동시에 계산하며, 각 배치에서 독립적인 행렬 곱이 이루어진다.\n",
    "- torch.matmul()은 상황에 맞게 벡터, 행렬, 배치 행렬 간의 연산을 자동으로 수행한다.\n",
    "- t7, t8는 서로 독립적인 배치 간의 곱, t9, t10는 고정된 행렬과 여러 배치 행렬 간의 곱이다."
   ],
   "id": "a19f7f4a75d292de"
  },
  {
   "cell_type": "markdown",
   "id": "f8cffcd07d9fa8",
   "metadata": {},
   "source": [
    "# **i_tensor_broadcasting.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f383ca17d4a78ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:41:51.817750Z",
     "start_time": "2024-09-21T04:41:51.786533Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "cfd0dd7fc98349df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:09:31.934953Z",
     "start_time": "2024-09-21T05:09:31.903587Z"
    }
   },
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])              ##1차원 텐서\n",
    "t2 = 2.0                                        ##스칼라 값\n",
    "print(t1 * t2)                                  ##t1의 각 요소에 t2(스칼라 값)을 곱함\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])   ##2차원 텐서\n",
    "t4 = torch.tensor([4, 5])                       ##1차원 텐서\n",
    "print(t3 - t4)                                  ##t3에서 t4를 브로드캐스팅하여 뺌. t4를 각행에 반복 적용\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])         ##2차원 텐서\n",
    "print(t5 + 2.0)  # t5.add(2.0)                  ##t5의 갹 요소에 +2.0\n",
    "print(t5 - 2.0)  # t5.sub(2.0)                  ##t5의 갹 요소에 -2.0\n",
    "print(t5 * 2.0)  # t5.mul(2.0)                  ##t5의 갹 요소에 *2.0\n",
    "print(t5 / 2.0)  # t5.div(2.0)                  ##t5의 갹 요소에 /2.0\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 텐서는 차원이 달라도 브로드캐스팅을 통해 사칙연산이 가능하다.",
   "id": "8afde9b4f0de605d"
  },
  {
   "cell_type": "code",
   "id": "d6d7583cd9e0c7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:12:22.089017Z",
     "start_time": "2024-09-21T05:12:22.058275Z"
    }
   },
   "source": [
    "def normalize(x):\n",
    "  return x / 255        ##보통 이미지 데이터를 정규화할 때 사용하는 방식(ex. 픽셀값)\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)     ##[3, 28, 28] 사이즈의 무작위 텐서(ex. 3채널{RGB} 이미지)\n",
    "print(normalize(t6).size())     ##정규화한 텐서의 크기 출력\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- nomalize() 함수를 통해 값들을 정규화시킬 수 있다.\n",
    "- 정규화하면, 값만 정규화될 뿐 텐서의 크기에는 영향을 주지 않는다."
   ],
   "id": "f3b6117330a65f0f"
  },
  {
   "cell_type": "code",
   "id": "14bf07e74a350f62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:18:55.530558Z",
     "start_time": "2024-09-21T05:18:55.516050Z"
    }
   },
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "                    ##[[1+3, 2+1], [0+3, 3+1]]\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "                    ##[[1+5, 2+5], [0+2, 3+2]]\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "                    ##[[3+5, 1+5], [3+2, 1+2]]\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "                    ##[[1+7, 2+7], [0+7, 3+7]]\n",
    "\n",
    "print(\"#\" * 50, 5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 각 텐서의 크기가 다르지만, 브로드캐스팅에 의해 자동으로 크기가 맞춰져 연산이 수행된다.",
   "id": "db5859f67acb2ac0"
  },
  {
   "cell_type": "code",
   "id": "8fb9b59db2bad161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:27:12.768451Z",
     "start_time": "2024-09-21T05:27:12.743017Z"
    }
   },
   "source": [
    "t11 = torch.ones(   4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(   4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(   4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(   3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(   3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(      1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(   3, 1, 1)         ##에러 발생 이유: 어느 한쪽이 1이거나 서로 같아야 함\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 브로드캐스팅 규칙은 텐서의 차원 크기가 1이거나 존재하지 않을 때 자동으로 복제하여 연산을 가능하게 만든다.\n",
    "- 위 코드에서처럼 마지막 차원부터 확인하면 편하다. 어느 한쪽이 1이거나, 서로 같으면 연산이 가능하고, 어느 한쪽이 가지고 있지 않은 차원을 다른 쪽이 가지고 있다면 그 차원의 사이즈를 그대로 가져온다.\n",
    "- t25, t26처럼 같은 차원이 2와 3으로 다르면 연산이 불가능하다."
   ],
   "id": "9e00e1a0638aacdc"
  },
  {
   "cell_type": "code",
   "id": "f9a7c527e167e9c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:27:16.880609Z",
     "start_time": "2024-09-21T05:27:16.843979Z"
    }
   },
   "source": [
    "t27 = torch.ones(4) * 5             ##[1., 1., 1., 1.]의 각 요소에 *5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)             ##[5., 5., 5., 5.]의 각 요소에 ^2\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)             ##a의 각 요소를 exp의 대응되는 요소로 제곱(1^1, 2^2, 3^3, 4^4)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.pow() 함수는 제곱을 위해 사용한다.",
   "id": "e4c03f5ea7add943"
  },
  {
   "cell_type": "markdown",
   "id": "83ea60b4c5fca2c9",
   "metadata": {},
   "source": [
    "# **j_tensor_indexing_slicing.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd3706b30a2e4dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T05:44:30.406121Z",
     "start_time": "2024-09-21T05:44:30.392114Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "36ce09bea46acb0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:04:28.697713Z",
     "start_time": "2024-09-21T06:04:28.666266Z"
    }
   },
   "source": [
    "x = torch.tensor(\n",
    "  [[ 0,  1,  2,  3,  4],\n",
    "   [ 5,  6,  7,  8,  9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "                                                ##교집합을 찾는다고 생각하면 된다\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])      ##1행\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])        ##모든 행, 1열의 교집합\n",
    "print(x[1, 2])  # >>> tensor(7)                 ##1행, 2열의 교집합\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)        ##모든 행, 마지막 열의 교집합\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])    ##1행~마지막행 \n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])    ##1행~마지막행, 3열~마지막열의 교집합\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "\n",
    "y = torch.zeros((6, 6))         ##모두 0.인 6x6 행렬\n",
    "y[1:4, 2] = 1                   ##1행~3행, 2열의 교집합을 1로 변경\n",
    "print(y)                        \n",
    "\n",
    "print(y[1:4, 1:4])              ##1행~3행, 1열~3열의 교집합을 출력\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])                    ##0행~1행\n",
    "print(z[1:, 1:3])               ##1행~마지막행, 1열~2열의 교집합\n",
    "print(z[:, 1:])                 ##모든 행, 1열~마지막열의 교집합\n",
    "\n",
    "z[1:, 1:3] = 0                  ##1행~마지막행, 1열~2열의 교집합을 0으로 변경\n",
    "print(z)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- ':'기호 -> 모든 행 또는 모든 열\n",
    "- 숫자 (ex.3) -> 그 행 또는 그 열 (ex.3행 또는 3열)\n",
    "- -1 -> 마지막 행 또는 마지막 열 의미\n",
    "- 숫자+':' (ex.3:) -> 숫자행 포함 그 이후 또는 숫자열 포함 그 이후 (ex.3행\\~마지막행 또는 3열\\~마지막열)\n",
    "- 숫자1+':'+숫자2 (ex.1:4) -> 숫자1행\\~(숫자2 - 1)행 또는 숫자1열\\~(숫자2 - 1)열 (ex.1행\\~3행 또는 1열\\~3열)\n",
    "- ':'+숫자 (ex.:2) -> 0행\\~(숫자-1)행 또는 0열\\~(숫자-1)열 (ex. 0행\\~1행 또는 0열\\~1열)"
   ],
   "id": "43cb38db67cdc1b6"
  },
  {
   "cell_type": "markdown",
   "id": "6f63da99c1b70d91",
   "metadata": {},
   "source": [
    "# **k_tensor_reshaping.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "7dd8079365cd4f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:07:51.126516Z",
     "start_time": "2024-09-21T06:07:51.098703Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "4c1fa2d4202aea4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:09:50.042881Z",
     "start_time": "2024-09-21T06:09:50.027363Z"
    }
   },
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 데이터는 변경되지 않고, shape만 바뀐다.\n",
    "- 연속적인 텐서: view()만 사용할 수 있고, 메모리 참조를 한다.\n",
    "- 불연속적 텐서: view(), reshape() 모두 사용할 수 있고, 메모리 카피를 한다."
   ],
   "id": "3502dfb92e7d491a"
  },
  {
   "cell_type": "code",
   "id": "b38f7beda2d6474a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:28:31.031803Z",
     "start_time": "2024-09-21T06:28:31.017793Z"
    }
   },
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- squeeze(dim): 괄호 안에 dim을 지정하지 않으면 크기가 1인 차원을 모두 없애고, dim을 지정하면 그 차원만 없앤다.\n",
    "- unsqueeze(dim): 괄호 안에 dim을 지정해야만 하고, dim을 지정하면 그 위치에 차원을 추가한다."
   ],
   "id": "3e67d62cfb5f7d80"
  },
  {
   "cell_type": "code",
   "id": "a4e212aa784ae90d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:34:00.853675Z",
     "start_time": "2024-09-21T06:34:00.830401Z"
    }
   },
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)       ##1번째 차원 이상의 모든 차원이 평탄화, 즉 2차원 텐서로 변환\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.flatten(): 옵션이 없다면 텐서를 1차원으로 평탄화해준다. start_dim을 설정하면 평탄화할 차원의 시작점을 조정할 수 있다.",
   "id": "b7e11d37580a365d"
  },
  {
   "cell_type": "code",
   "id": "f0972cc30a79de99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:43:36.283901Z",
     "start_time": "2024-09-21T06:43:36.261231Z"
    }
   },
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "print()             ##가독성을 위해 추가한 코드\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- torch.permute(input, dims): 텐서의 차원의 순서를 변경하는데 사용, 각 차원을 원하는 순서로 재배열\n",
    "- torch.transpose(tensor, dim0, dim1): 텐서를 전치하는데 사용, dim0 차원과 dim1 차원이 바뀜, 모든 차원에 대해 사용 가능\n",
    "- torch.t(tensor): 텐서를 전치하는데 사용, 2차원 텐서에 사용 가능"
   ],
   "id": "f7be821ee8e93c13"
  },
  {
   "cell_type": "markdown",
   "id": "aa562be6d56d058e",
   "metadata": {},
   "source": [
    "# **l_tensor_concat.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bcad7e8e7afc2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:48:17.144985Z",
     "start_time": "2024-09-21T06:48:17.139477Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "1cca3487b9f6043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:55:03.603999Z",
     "start_time": "2024-09-21T06:55:03.573247Z"
    }
   },
   "source": [
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)     ##1번째 차원에 concat -> [2, 6, 3]\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)         ##0번째 차원에 concat -> [8]\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.cat(tensors, dim) == torch.concat(tensors, dim)",
   "id": "908b562fa230f89a"
  },
  {
   "cell_type": "code",
   "id": "47187e40b2733563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:58:54.569157Z",
     "start_time": "2024-09-21T06:58:54.551596Z"
    }
   },
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)            ##0번째 차원에 concat -> [4, 3]\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)            ##1번째 차원에 concat -> [2, 6]\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "8ff3a3e31967e34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:59:06.074669Z",
     "start_time": "2024-09-21T06:59:06.058947Z"
    }
   },
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)         ##0번째 차원에 concat -> [6, 3]\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)         ##1번째 차원에 concat -> [2, 9]\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "b0ebbca1aab42dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T06:59:15.440678Z",
     "start_time": "2024-09-21T06:59:15.409405Z"
    }
   },
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)              ##0번째 차원에 concat -> [2, 2, 3]\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)              ##1번째 차원에 concat -> [1, 4, 3]\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)              ##2번째 차원에 concat -> [1, 2, 6]\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.cat(tensors, dim) 함수는 주어진 텐서들을 특정 차원에서 연결하여 새로운 텐서를 생성한다.",
   "id": "2779cd8d9604c38f"
  },
  {
   "cell_type": "markdown",
   "id": "64fb880e5d627ce",
   "metadata": {},
   "source": [
    "# **m_tensor_stacking.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2cec304f18b9057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T07:05:02.261368Z",
     "start_time": "2024-09-21T07:05:02.245454Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "3a464712cbf17ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T07:07:12.257766Z",
     "start_time": "2024-09-21T07:07:12.242094Z"
    }
   },
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])           ##[2, 3]\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])        ##[2, 3]\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)       ##새 차원을 만들고 concat을 하는 것이라 생각하면 됨\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))           ##t3과 t4는 동일함\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "1e0773314ae8d359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T07:08:18.233303Z",
     "start_time": "2024-09-21T07:08:18.208928Z"
    }
   },
   "source": [
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.stack(tensors, dim): dim 위치에 새 차원을 하나 만들고, concat을 하는 것과 동일한 결과를 갖는다.",
   "id": "c19ddc54947be6d4"
  },
  {
   "cell_type": "markdown",
   "id": "3a20afce18aae397",
   "metadata": {},
   "source": [
    "# **n_tensor_vstack_hstack.py**"
   ]
  },
  {
   "cell_type": "code",
   "id": "45be9be4a7667a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T07:13:29.327270Z",
     "start_time": "2024-09-21T07:13:29.295382Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "12a9ef292501d822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T07:15:22.738012Z",
     "start_time": "2024-09-21T07:15:22.706590Z"
    }
   },
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))             ##수직으로 쌓는다고 생각하면 됨\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.vstack(): 주어진 텐서를 수직으로 쌓아 새로운 텐서를 만든다.",
   "id": "b5b44e169107e84c"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "588da7418f2c10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))              ##수평으로 붙인다고 생각하면 됨\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- torch.hstack(): 주어진 텐서를 수평으로 쌓아 새로운 텐서를 만든다.",
   "id": "9399d124aa3901d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **숙제 후기**",
   "id": "987a2dd2d116b5a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 자료로 나눠주신 jupyter notebook 단축키 외에, markdown의 글씨 크기 설정, 글씨 두께 설정 등 기본적인 사용법에 대해서 알 수 있었다.\n",
    "- 원본 코드에 작성된 주석과 구분할 수 있도록 ##으로 주석을 달아놓았다.\n",
    "- 특강을 듣고 이 코드를 다시 보니, 이 코드들이 제일 기본적이면서 중요한 코드라는 생각이 들었다.\n",
    "- 코드에 써넣은 주석과, 코드 아래 markdown으로 작성한 설명을 이 정도로 쓰면 괜찮은 것인지 궁금하다.\n",
    "- 이렇게 과제에 주석과 설명을 잘 적어놓으면 나중에 다시 찾아볼 때나 궁금한 게 생겼을 때 매우 도움이 될 것 같다."
   ],
   "id": "18e778e4a249691d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
